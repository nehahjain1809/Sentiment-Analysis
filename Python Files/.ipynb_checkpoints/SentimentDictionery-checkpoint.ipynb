{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math, re, string, requests, json\n",
    "from itertools import product\n",
    "from nltk.corpus import stopwords\n",
    "import pymysql.cursors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connect successful!!\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database.\n",
    "connection = pymysql.connect(host='127.0.0.1',\n",
    "                             user='root',\n",
    "                             password='neha',                             \n",
    "                             db='simpsons',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    " \n",
    "print (\"connect successful!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = open(\"C:/Users/nehah/Desktop/Simpsons Project/Datasets/Simpsons Lexicon.csv\",\"r\")\n",
    "lex_dict = {}\n",
    "for line in file:\n",
    "    (word, measure) = line.strip().split(',')[0:2]\n",
    "    lex_dict[word] = float(measure)\n",
    "#lex_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B_INCR = 0.293\n",
    "B_DECR = -0.293\n",
    "N_SCALAR = -0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEGATE = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    " \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    " \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    " \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    " \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    " \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    " \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    " \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPECIAL_CASE_IDIOMS = {\"the shit\": 3, \"the bomb\": 3, \"bad ass\": 1.5, \"yeah right\": -2,\n",
    "                       \"cut the mustard\": 2, \"kiss of death\": -1.5, \"hand to mouth\": -2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOSTER_DICT = \\\n",
    "{\"absolutely\": B_INCR, \"amazingly\": B_INCR, \"awfully\": B_INCR, \"completely\": B_INCR, \"considerably\": B_INCR,\n",
    " \"decidedly\": B_INCR, \"deeply\": B_INCR, \"effing\": B_INCR, \"enormously\": B_INCR,\n",
    " \"entirely\": B_INCR, \"especially\": B_INCR, \"exceptionally\": B_INCR, \"extremely\": B_INCR,\n",
    " \"fabulously\": B_INCR, \"flipping\": B_INCR, \"flippin\": B_INCR,\n",
    " \"fricking\": B_INCR, \"frickin\": B_INCR, \"frigging\": B_INCR, \"friggin\": B_INCR, \"fully\": B_INCR, \"fucking\": B_INCR,\n",
    " \"greatly\": B_INCR, \"hella\": B_INCR, \"highly\": B_INCR, \"hugely\": B_INCR, \"incredibly\": B_INCR,\n",
    " \"intensely\": B_INCR, \"majorly\": B_INCR, \"more\": B_INCR, \"most\": B_INCR, \"particularly\": B_INCR,\n",
    " \"purely\": B_INCR, \"quite\": B_INCR, \"really\": B_INCR, \"remarkably\": B_INCR,\n",
    " \"so\": B_INCR, \"substantially\": B_INCR,\n",
    " \"thoroughly\": B_INCR, \"totally\": B_INCR, \"tremendously\": B_INCR,\n",
    " \"uber\": B_INCR, \"unbelievably\": B_INCR, \"unusually\": B_INCR, \"utterly\": B_INCR,\n",
    " \"very\": B_INCR,\n",
    " \"almost\": B_DECR, \"barely\": B_DECR, \"hardly\": B_DECR, \"just enough\": B_DECR,\n",
    " \"kind of\": B_DECR, \"kinda\": B_DECR, \"kindof\": B_DECR, \"kind-of\": B_DECR,\n",
    " \"less\": B_DECR, \"little\": B_DECR, \"marginally\": B_DECR, \"occasionally\": B_DECR, \"partly\": B_DECR,\n",
    " \"scarcely\": B_DECR, \"slightly\": B_DECR, \"somewhat\": B_DECR,\n",
    " \"sort of\": B_DECR, \"sorta\": B_DECR, \"sortof\": B_DECR, \"sort-of\": B_DECR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(normalized_text):\n",
    "    nopunc = [char for char in normalized_text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    list_without_stopwords = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    return ' '.join(list_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scalar_inc_dec(word, valence):\n",
    "   \n",
    "    scalar = 0.0\n",
    "    \n",
    "    if word in BOOSTER_DICT:\n",
    "        scalar = BOOSTER_DICT[word]\n",
    "        if valence < 0:\n",
    "            scalar *= -1\n",
    "   \n",
    "    return scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negated(input_words, include_nt=True):\n",
    "    \"\"\"\n",
    "    Determine if input contains negation words\n",
    "    \"\"\"\n",
    "    neg_words = []\n",
    "    neg_words.extend(NEGATE)\n",
    "    for word in neg_words:\n",
    "        if word in input_words:\n",
    "            return True\n",
    "    if include_nt:\n",
    "        for word in input_words:\n",
    "            if \"n't\" in word:\n",
    "                return True\n",
    "    if \"least\" in input_words:\n",
    "        i = input_words.index(\"least\")\n",
    "        if i > 0 and input_words[i-1] != \"at\":\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _never_check(valence, words, start_i, i):\n",
    "        if start_i == 0:\n",
    "            if negated([words[i-1]]):\n",
    "                    valence = valence*N_SCALAR\n",
    "        if start_i == 1:\n",
    "            if words[i-2] == \"never\" and\\\n",
    "               (words[i-1] == \"so\" or\n",
    "                words[i-1] == \"this\"):\n",
    "                valence = valence*1.5\n",
    "            elif negated([words[i-(start_i+1)]]):\n",
    "                valence = valence*N_SCALAR\n",
    "        if start_i == 2:\n",
    "            if words[i-3] == \"never\" and \\\n",
    "               (words[i-2] == \"so\" or words[i-2] == \"this\") or \\\n",
    "               (words[i-1] == \"so\" or words[i-1] == \"this\"):\n",
    "                valence = valence*1.25\n",
    "            elif negated([words[i-(start_i+1)]]):\n",
    "                valence = valence*N_SCALAR\n",
    "        return valence\n",
    "\n",
    "def _idioms_check(valence, words, i):\n",
    "        onezero = \"{0} {1}\".format(words[i-1], words[i])\n",
    "\n",
    "        twoonezero = \"{0} {1} {2}\".format(words[i-2],\n",
    "                                       words[i-1], words[i])\n",
    "\n",
    "        twoone = \"{0} {1}\".format(words[i-2], words[i-1])\n",
    "\n",
    "        threetwoone = \"{0} {1} {2}\".format(words[i-3],\n",
    "                                        words[i-2], words[i-1])\n",
    "\n",
    "        threetwo = \"{0} {1}\".format(words[i-3], words[i-2])\n",
    "\n",
    "        sequences = [onezero, twoonezero, twoone, threetwoone, threetwo]\n",
    "\n",
    "        for seq in sequences:\n",
    "            if seq in SPECIAL_CASE_IDIOMS:\n",
    "                valence = SPECIAL_CASE_IDIOMS[seq]\n",
    "                break\n",
    "\n",
    "        if len(words)-1 > i:\n",
    "            zeroone = \"{0} {1}\".format(words[i], words[i+1])\n",
    "            if zeroone in SPECIAL_CASE_IDIOMS:\n",
    "                valence = SPECIAL_CASE_IDIOMS[zeroone]\n",
    "        if len(words)-1 > i+1:\n",
    "            zeroonetwo = \"{0} {1} {2}\".format(words[i], words[i+1], words[i+2])\n",
    "            if zeroonetwo in SPECIAL_CASE_IDIOMS:\n",
    "                valence = SPECIAL_CASE_IDIOMS[zeroonetwo]\n",
    "\n",
    "        # check for booster/dampener bi-grams such as 'sort of' or 'kind of'\n",
    "        if threetwo in BOOSTER_DICT or twoone in BOOSTER_DICT:\n",
    "            valence = valence+B_DECR\n",
    "        return valence\n",
    "\n",
    "def _least_check(valence, words, i):\n",
    "        # check for negation case using \"least\"\n",
    "        if i > 1 and words[i-1].lower() not in lex_dict \\\n",
    "           and words[i-1].lower() == \"least\":\n",
    "            if words[i-2].lower() != \"at\" and words[i-2].lower() != \"very\":\n",
    "                valence = valence*N_SCALAR\n",
    "        elif i > 0 and words[i-1].lower() not in lex_dict \\\n",
    "             and words[i-1].lower() == \"least\":\n",
    "            valence = valence*N_SCALAR\n",
    "        return valence\n",
    "\n",
    "def _but_check(words, sentiments):\n",
    "        # check for modification in sentiment due to contrastive conjunction 'but'\n",
    "        if 'but' in words or 'BUT' in words:\n",
    "            try:\n",
    "                bi = words.index('but')\n",
    "            except ValueError:\n",
    "                bi = words.index('BUT')\n",
    "            for sentiment in sentiments:\n",
    "                si = sentiments.index(sentiment)\n",
    "                if si < bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment*0.5)\n",
    "                elif si > bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment*1.5)\n",
    "        return sentiments\n",
    "    \n",
    "def _least_check(valence, words, i):\n",
    "        # check for negation case using \"least\"\n",
    "        if i > 1 and words[i-1].lower() not in lex_dict \\\n",
    "           and words[i-1].lower() == \"least\":\n",
    "            if words[i-2].lower() != \"at\" and words[i-2].lower() != \"very\":\n",
    "                valence = valence*N_SCALAR\n",
    "        elif i > 0 and words[i-1].lower() not in lex_dict \\\n",
    "             and words[i-1].lower() == \"least\":\n",
    "            valence = valence*N_SCALAR\n",
    "        return valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_valence(valence,item, i, sentiments,words):\n",
    "        \n",
    "        if item in lex_dict:\n",
    "            #get the sentiment valence\n",
    "            valence = lex_dict[item]\n",
    "\n",
    "            for start_i in range(0,3):\n",
    "                if i > start_i and words[i-(start_i+1)].lower() not in lex_dict:\n",
    "                    # dampen the scalar modifier of preceding words\n",
    "                    # (excluding the ones that immediately preceed the item) based\n",
    "                    # on their distance from the current item.\n",
    "                    #print(i)\n",
    "                    s = scalar_inc_dec(words[i-(start_i+1)], valence)\n",
    "                    if start_i == 1 and s != 0:\n",
    "                        s = s*0.95\n",
    "                    if start_i == 2 and s != 0:\n",
    "                        s = s*0.9\n",
    "                    valence = valence+s\n",
    "                    #print(valence)\n",
    "                    valence = _never_check(valence, words, start_i, i)\n",
    "                    if start_i == 2:\n",
    "                        valence = _idioms_check(valence, words, i)\n",
    "                        \n",
    "            valence = _least_check(valence, words, i)\n",
    "\n",
    "        sentiments.append(valence)\n",
    "        return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_valence(sentiments, text):\n",
    "        if sentiments:\n",
    "            sum_s = float(sum(sentiments))\n",
    "\n",
    "            compound = normalize(sum_s)\n",
    "            # discriminate between positive, negative and neutral sentiment scores\n",
    "            pos_sum, neg_sum, neu_count = _sift_sentiment_scores(sentiments)\n",
    "\n",
    "            total = pos_sum + math.fabs(neg_sum) + neu_count\n",
    "            pos = math.fabs(pos_sum / total)\n",
    "            neg = math.fabs(neg_sum / total)\n",
    "            neu = math.fabs(neu_count / total)\n",
    "\n",
    "        else:\n",
    "            compound = 0.0\n",
    "            pos = 0.0\n",
    "            neg = 0.0\n",
    "            neu = 0.0\n",
    "\n",
    "        sentiment_dict = \\\n",
    "            {\"neg\" : round(neg, 3),\n",
    "             \"neu\" : round(neu, 3),\n",
    "             \"pos\" : round(pos, 3),\n",
    "             \"compound\" : round(compound, 4)}\n",
    "\n",
    "        return sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_scores(text):\n",
    "    #text=clean_text(text)\n",
    "    \n",
    "    sentiments = []\n",
    "    words=text.split()\n",
    "    \n",
    "    for item in words:\n",
    "        valence=0\n",
    "        i=words.index(item)\n",
    "        #print(i)\n",
    "        sentiments = sentiment_valence(valence,item, i, sentiments,words)\n",
    "        sentiments = _but_check(words, sentiments)\n",
    "    valence_dict = score_valence(sentiments, text)\n",
    "\n",
    "    return valence_dict['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score=polarity_scores('thank')\n",
    "#print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(score, alpha=15):\n",
    "    \"\"\"\n",
    "    Normalize the score to be between -1 and 1 using an alpha that\n",
    "    approximates the max expected value\n",
    "    \"\"\"\n",
    "    norm_score = score/math.sqrt((score*score) + alpha)\n",
    "    if norm_score < -1.0: \n",
    "        return -1.0\n",
    "    elif norm_score > 1.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return norm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _sift_sentiment_scores(sentiments):\n",
    "        # want separate positive versus negative sentiment scores\n",
    "        pos_sum = 0.0\n",
    "        neg_sum = 0.0\n",
    "        neu_count = 0\n",
    "        for sentiment_score in sentiments:\n",
    "            if sentiment_score > 0:\n",
    "                pos_sum += (float(sentiment_score) +1) # compensates for neutral words that are counted as 1\n",
    "            if sentiment_score < 0:\n",
    "                neg_sum += (float(sentiment_score) -1) # when used with math.fabs(), compensates for neutrals\n",
    "            if sentiment_score == 0:\n",
    "                neu_count += 1\n",
    "        return pos_sum, neg_sum, neu_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
