{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nehah\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "C:\\Users\\nehah\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import pymysql\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "from gcloud import storage\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import seaborn as sns\n",
    "import json\n",
    "from decimal import Decimal\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import statsmodels.api as sm\n",
    "from wordcloud import WordCloud \n",
    "import math, re, string, requests, json\n",
    "from itertools import product\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "plt.rcParams['figure.figsize'] = (30, 6)\n",
    "from afinn import Afinn\n",
    "afinn=Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connect successful!!\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database.\n",
    "connection = pymysql.connect(host='127.0.0.1',\n",
    "                             user='root',\n",
    "                             password='neha',                             \n",
    "                             db='simpsons',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    " \n",
    "print (\"connect successful!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(normalized_text):\n",
    "    nopunc = [char for char in normalized_text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    list_without_stopwords = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    return ' '.join(list_without_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sentiment calculator\n",
    "\n",
    "# definig constants\n",
    "\n",
    "B_INCR = 0.293\n",
    "B_DECR = -0.293\n",
    "N_SCALAR = -0.74\n",
    "\n",
    "NEGATE = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    " \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    " \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    " \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    " \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    " \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    " \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    " \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
    "\n",
    "SPECIAL_CASE_IDIOMS = {\"the shit\": 3, \"the bomb\": 3, \"bad ass\": 1.5, \"yeah right\": -2,\n",
    "                        \"kiss of death\": -1.5, \"shut your mouth\": -2, \"hang your head\": -2,\n",
    "                       \"you jerk\": -2.5, \"dee-lish\": 2,\"black private dick\":-4, \"damn right\": 3,\n",
    "                      \"shut up\": -1.5}\n",
    "\n",
    "BOOSTER_DICT = \\\n",
    "{\"absolutely\": B_INCR, \"amazingly\": B_INCR, \"awfully\": B_INCR, \"completely\": B_INCR, \"considerably\": B_INCR,\n",
    " \"decidedly\": B_INCR, \"deeply\": B_INCR, \"effing\": B_INCR, \"enormously\": B_INCR,\n",
    " \"entirely\": B_INCR, \"especially\": B_INCR, \"exceptionally\": B_INCR, \"extremely\": B_INCR,\n",
    " \"fabulously\": B_INCR, \"flipping\": B_INCR, \"flippin\": B_INCR,\n",
    " \"fricking\": B_INCR, \"frickin\": B_INCR, \"frigging\": B_INCR, \"friggin\": B_INCR, \"fully\": B_INCR, \"fucking\": B_INCR,\n",
    " \"greatly\": B_INCR, \"hella\": B_INCR, \"highly\": B_INCR, \"hugely\": B_INCR, \"incredibly\": B_INCR,\n",
    " \"intensely\": B_INCR, \"majorly\": B_INCR, \"more\": B_INCR, \"most\": B_INCR, \"particularly\": B_INCR,\n",
    " \"purely\": B_INCR, \"quite\": B_INCR, \"really\": B_INCR, \"remarkably\": B_INCR,\n",
    " \"so\": B_INCR, \"substantially\": B_INCR,\n",
    " \"thoroughly\": B_INCR, \"totally\": B_INCR, \"tremendously\": B_INCR,\n",
    " \"uber\": B_INCR, \"unbelievably\": B_INCR, \"unusually\": B_INCR, \"utterly\": B_INCR,\n",
    " \"very\": B_INCR,\n",
    " \"almost\": B_DECR, \"barely\": B_DECR, \"hardly\": B_DECR, \"just enough\": B_DECR,\n",
    " \"kind of\": B_DECR, \"kinda\": B_DECR, \"kindof\": B_DECR, \"kind-of\": B_DECR,\n",
    " \"less\": B_DECR, \"little\": B_DECR, \"marginally\": B_DECR, \"occasionally\": B_DECR, \"partly\": B_DECR,\n",
    " \"scarcely\": B_DECR, \"slightly\": B_DECR, \"somewhat\": B_DECR,\n",
    " \"sort of\": B_DECR, \"sorta\": B_DECR, \"sortof\": B_DECR, \"sort-of\": B_DECR,\n",
    "\"unbeliev\": B_INCR , \"better\":B_INCR ,\"great\": B_INCR }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scalar_inc_dec(word, valence):\n",
    "   \n",
    "    scalar = 0.0\n",
    "    \n",
    "    if word in BOOSTER_DICT:\n",
    "        scalar = BOOSTER_DICT[word]\n",
    "        if valence < 0:\n",
    "            scalar *= -1\n",
    "   \n",
    "    return scalar\n",
    "\n",
    "def negated(input_words, include_nt=True):\n",
    "    \"\"\"\n",
    "    Determine if input contains negation words\n",
    "    \"\"\"\n",
    "    neg_words = []\n",
    "    neg_words.extend(NEGATE)\n",
    "    for word in neg_words:\n",
    "        if word in input_words:\n",
    "            return True\n",
    "    if include_nt:\n",
    "        for word in input_words:\n",
    "            if \"n't\" in word:\n",
    "                return True\n",
    "    if \"least\" in input_words:\n",
    "        i = input_words.index(\"least\")\n",
    "        if i > 0 and input_words[i-1] != \"at\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _never_check(valence, words, start_i, i):\n",
    "        if start_i == 0:\n",
    "            if negated([words[i-1]]):\n",
    "                    valence = valence*N_SCALAR\n",
    "        return valence\n",
    "\n",
    "\n",
    "def _least_check(valence, words, i):\n",
    "        # check for negation case using \"least\"\n",
    "        if i > 1 and words[i-1].lower() not in lex_dict \\\n",
    "           and words[i-1].lower() == \"least\":\n",
    "            if words[i-2].lower() != \"at\" and words[i-2].lower() != \"very\":\n",
    "                valence = valence*N_SCALAR\n",
    "        elif i > 0 and words[i-1].lower() not in lex_dict \\\n",
    "             and words[i-1].lower() == \"least\":\n",
    "            valence = valence*N_SCALAR\n",
    "        return valence\n",
    "\n",
    "\n",
    "def _but_check(words, sentiments):\n",
    "        # check for modification in sentiment due to contrastive conjunction 'but'\n",
    "        if 'but' in words or 'BUT' in words:\n",
    "            try:\n",
    "                bi = words.index('but')\n",
    "            except ValueError:\n",
    "                bi = words.index('BUT')\n",
    "            for sentiment in sentiments:\n",
    "                si = sentiments.index(sentiment)\n",
    "                if si < bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment*0.5)\n",
    "                elif si > bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment*1.5)\n",
    "        return sentiments\n",
    "    \n",
    "\n",
    "def normalize(score, alpha=15):\n",
    "    \"\"\"\n",
    "    Normalize the score to be between -1 and 1 using an alpha that\n",
    "    approximates the max expected value\n",
    "    \"\"\"\n",
    "    norm_score = score/math.sqrt((score*score) + alpha)\n",
    "    if norm_score < -1.0: \n",
    "        return -1.0\n",
    "    elif norm_score > 1.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return norm_score\n",
    "\n",
    "def _sift_sentiment_scores(sentiments):\n",
    "        # want separate positive versus negative sentiment scores\n",
    "        pos_sum = 0.0\n",
    "        neg_sum = 0.0\n",
    "        neu_count = 0\n",
    "        for sentiment_score in sentiments:\n",
    "            if sentiment_score > 0:\n",
    "                pos_sum += (float(sentiment_score) +1) # compensates for neutral words that are counted as 1\n",
    "            if sentiment_score < 0:\n",
    "                neg_sum += (float(sentiment_score) -1) # when used with math.fabs(), compensates for neutrals\n",
    "            if sentiment_score == 0:\n",
    "                neu_count += 1\n",
    "        return pos_sum, neg_sum, neu_count\n",
    "\n",
    "def sentiment_valence(valence,item, i, sentiments,words):\n",
    "        \n",
    "        if item in lex_dict:\n",
    "            #get the sentiment valence\n",
    "            valence = lex_dict[item]\n",
    "\n",
    "            for start_i in range(0,3):\n",
    "                if i > start_i and words[i-(start_i+1)].lower() not in lex_dict:\n",
    "                    # dampen the scalar modifier of preceding words\n",
    "                    # (excluding the ones that immediately preceed the item) based\n",
    "                    # on their distance from the current item.\n",
    "                    #print(i)\n",
    "                    s = scalar_inc_dec(words[i-(start_i+1)], valence)\n",
    "                    if start_i == 1 and s != 0:\n",
    "                        s = s*0.95\n",
    "                    if start_i == 2 and s != 0:\n",
    "                        s = s*0.9\n",
    "                    valence = valence+s\n",
    "                    #print(valence)\n",
    "                    valence = _never_check(valence, words, start_i, i)\n",
    "                    #if start_i == 2:\n",
    "                        #valence = _idioms_check(valence, words, i)\n",
    "                        \n",
    "            valence = _least_check(valence, words, i)\n",
    "\n",
    "        sentiments.append(valence)\n",
    "        return sentiments\n",
    "\n",
    "def score_valence(sentiments, text):\n",
    "        if sentiments:\n",
    "            sum_s = float(sum(sentiments))\n",
    "\n",
    "            compound = normalize(sum_s)\n",
    "            # discriminate between positive, negative and neutral sentiment scores\n",
    "            pos_sum, neg_sum, neu_count = _sift_sentiment_scores(sentiments)\n",
    "\n",
    "            total = pos_sum + math.fabs(neg_sum) + neu_count\n",
    "            pos = math.fabs(pos_sum / total)\n",
    "            neg = math.fabs(neg_sum / total)\n",
    "            neu = math.fabs(neu_count / total)\n",
    "\n",
    "        else:\n",
    "            compound = 0.0\n",
    "            pos = 0.0\n",
    "            neg = 0.0\n",
    "            neu = 0.0\n",
    "\n",
    "        sentiment_dict = \\\n",
    "            {\"neg\" : round(neg, 3),\n",
    "             \"neu\" : round(neu, 3),\n",
    "             \"pos\" : round(pos, 3),\n",
    "             \"compound\" : round(compound, 4)}\n",
    "\n",
    "        return sentiment_dict\n",
    "\n",
    "\n",
    "def polarity_scores(text):\n",
    "    #text=clean_text(text)\n",
    "    \n",
    "    sentiments = []\n",
    "    words=text.split()\n",
    "    #words=[stemmer.stem(word) for word in words]\n",
    "    for item in words:\n",
    "        valence=0\n",
    "        i=words.index(item)\n",
    "        #print(i)\n",
    "        sentiments = sentiment_valence(valence,item, i, sentiments,words)\n",
    "        sentiments = _but_check(words, sentiments)\n",
    "    valence_dict = score_valence(sentiments, text)\n",
    "\n",
    "    return valence_dict['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the lexicon file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#C:\\Users\\nehah\\Desktop\\Simpsons Project\\Datasets\n",
    "file1 = open(\"C:/Users/nehah/Desktop/Simpsons Project/Datasets/Simpsons Lexicon.csv\",\"r\")\n",
    "lex_dict = {}\n",
    "for line in file1:\n",
    "    (word, measure) = line.strip().split(',')[0:2]\n",
    "    lex_dict[word] = float(measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2                                        ooo care homer\n",
       "3                              there no time to be care\n",
       "4                                             were late\n",
       "7                              sorri excus us pardon me\n",
       "8     hey norman how it go so you got drag down here...\n",
       "9                                      pardon my galosh\n",
       "10    wasnt that wonder and now santa of mani land a...\n",
       "11                                        oh lisa class\n",
       "12    frohlich weihnachten -- that german for merri ...\n",
       "13    meri kurimasu i am hotseiosha a japanes priest...\n",
       "Name: normalized_text_stem, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_script_lines=pd.read_sql(\"select normalized_text from script_lines\",con=connection);\n",
    "\n",
    "df_script_lines=df_script_lines[df_script_lines['normalized_text'] != '']\n",
    "\n",
    "df_script_lines['normalized_text_stem'] =  [\" \".join([stemmer.stem(word) for word in sentence.split(\" \")]) for sentence in df_script_lines['normalized_text']]\n",
    "\n",
    "df_script_lines['sentiment_score']  =  df_script_lines['normalized_text'].map(polarity_scores)\n",
    "\n",
    "df_script_lines['normalized_text_stem'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(lex_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_words(normalized_text):\n",
    "    clean=[word for word in normalized_text.split(' ') if word in lex_dict.keys()]\n",
    "    return ' '.join(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_script_lines['normalized_text_stem1']=df_script_lines['normalized_text_stem'].map(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_script_lines=df_script_lines[df_script_lines['sentiment_score'] !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_script_lines=df_script_lines.sort_values(by=['sentiment_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment_score_to_category(score):\n",
    "    if(score>0):\n",
    "        return 'positive'\n",
    "    if(score<0):\n",
    "        return 'negative'\n",
    "    if(score==0):\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>normalized_text_stem</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>normalized_text_stem1</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96334</th>\n",
       "      <td>hello im the us constitution and im over two h...</td>\n",
       "      <td>hello im the us constitut and im over two hund...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fine wish</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>huh a very thoughtful gift but its a surprise ...</td>\n",
       "      <td>huh a veri thought gift but it a surpris you k...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>gift surpris beauti ill block</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34898</th>\n",
       "      <td>but i like that film biography idea a slick ho...</td>\n",
       "      <td>but i like that film biographi idea a slick ho...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>like evil like</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35047</th>\n",
       "      <td>no homer i wont make fun of you but i will sug...</td>\n",
       "      <td>no homer i wont make fun of you but i will sug...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no fun better</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10433</th>\n",
       "      <td>all right theres no check just a card but dont...</td>\n",
       "      <td>all right there no check just a card but dont ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no panic sure dear thank</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         normalized_text  \\\n",
       "96334  hello im the us constitution and im over two h...   \n",
       "2461   huh a very thoughtful gift but its a surprise ...   \n",
       "34898  but i like that film biography idea a slick ho...   \n",
       "35047  no homer i wont make fun of you but i will sug...   \n",
       "10433  all right theres no check just a card but dont...   \n",
       "\n",
       "                                    normalized_text_stem  sentiment_score  \\\n",
       "96334  hello im the us constitut and im over two hund...              1.0   \n",
       "2461   huh a veri thought gift but it a surpris you k...              1.0   \n",
       "34898  but i like that film biographi idea a slick ho...              1.0   \n",
       "35047  no homer i wont make fun of you but i will sug...              1.0   \n",
       "10433  all right there no check just a card but dont ...              1.0   \n",
       "\n",
       "               normalized_text_stem1 sentiment  \n",
       "96334                      fine wish  positive  \n",
       "2461   gift surpris beauti ill block  positive  \n",
       "34898                 like evil like  positive  \n",
       "35047                  no fun better  positive  \n",
       "10433       no panic sure dear thank  positive  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_script_lines['sentiment'] =  df_script_lines['sentiment_score'].map(sentiment_score_to_category)\n",
    "df_script_lines.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results show a clear glimpse of the original dialogue, the root words , the sentiment score and the words resulting for the sentiment.\n",
    "Though the booster words, negate words, 'but' and 'never' also affect the sentiment. as of now let us continue and check the accuracy.\n",
    "\n",
    "I will be considering the normalized_text_stem1 column as my predictor variable and score (positive or negative) as the column to be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train,text_test,sentiment_train,sentiment_test = train_test_split(df_script_lines['normalized_text_stem1'],df_script_lines['sentiment'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.61      0.15      0.25      9076\n",
      "   positive       0.63      0.94      0.75     14030\n",
      "\n",
      "avg / total       0.62      0.63      0.55     23106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=clean_text)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',MultinomialNB())\n",
    "])\n",
    "pipeline.fit(text_train,sentiment_train)\n",
    "sentiment_predictions = pipeline.predict(text_test)\n",
    "\n",
    "\n",
    "print(classification_report(sentiment_test,sentiment_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62871115727516658"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(sentiment_test, sentiment_predictions, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.80      0.61      0.69      9076\n",
      "   positive       0.78      0.90      0.84     14030\n",
      "\n",
      "avg / total       0.79      0.78      0.78     23106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=clean_text)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',DecisionTreeClassifier())\n",
    "])\n",
    "pipeline.fit(text_train,sentiment_train)\n",
    "sentiment_predictions = pipeline.predict(text_test)\n",
    "\n",
    "\n",
    "print(classification_report(sentiment_test,sentiment_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78486107504544278"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(sentiment_test, sentiment_predictions, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Random Forest classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.82      0.61      0.70      9076\n",
      "   positive       0.79      0.92      0.85     14030\n",
      "\n",
      "avg / total       0.80      0.80      0.79     23106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=clean_text)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',RandomForestClassifier())\n",
    "])\n",
    "pipeline.fit(text_train,sentiment_train)\n",
    "sentiment_predictions = pipeline.predict(text_test)\n",
    "\n",
    "\n",
    "print(classification_report(sentiment_test,sentiment_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79641651519085954"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(sentiment_test, sentiment_predictions, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5564,  3512],\n",
       "       [ 1192, 12838]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(sentiment_test, sentiment_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.70      0.40      0.51      9076\n",
      "   positive       0.70      0.89      0.78     14030\n",
      "\n",
      "avg / total       0.70      0.70      0.68     23106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=clean_text)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',SVC())\n",
    "])\n",
    "pipeline.fit(text_train,sentiment_train)\n",
    "sentiment_predictions = pipeline.predict(text_test)\n",
    "\n",
    "\n",
    "print(classification_report(sentiment_test,sentiment_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69864970137626592"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(sentiment_test, sentiment_predictions, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed that the best results 80% score has been obtained using the Random Forest Classifier which is more or less same as that obtained from Decision Tree.\n",
    "Hence, we can colclude that random forest classifier is thebest suit for our dataset as of now.\n",
    "There is more scope for accurate classification of this data so as to get better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
