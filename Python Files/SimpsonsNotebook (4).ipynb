{"cells":[{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport os\nimport csv\nimport seaborn as sns\nimport json\nfrom decimal import Decimal\nimport nltk\n#nltk.download()\nfrom nltk.corpus import stopwords\nimport string\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\nfrom afinn import Afinn\n# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n# analyser = SentimentIntensityAnalyzer()\nafinn=Afinn()\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import classification_report"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#Defining funtions to clean the raw text, calculate the sentiment score and categorize the sentiment into positive and negative"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["def clean_text(normalized_text):\n    nopunc = [char for char in normalized_text if char not in string.punctuation]\n    nopunc = ''.join(nopunc)\n    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["def text_to_sentiment_score(text):\n   return afinn.score(text)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["def sentiment_score_to_category(score):\n    if(score>0):\n        return 'positive'\n        #display('p')\n    if(score<0):\n        return 'negative'\n    if(score==0):\n        return 'neutral'"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["def convert_to_int(value):\n  return value"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#convert to supported Datatypes"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["maxUdf=udf(text_to_sentiment_score, FloatType())\nmaxUdf1=udf(sentiment_score_to_category, StringType())\nintegerUdf=udf(convert_to_int, IntegerType())\n#cleanUdf=udf(clean_text,StringType())"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# The dataset contains 300+ episodes data but for analysis purpose, I am using 4 epsiodes for streaming of dialogues spoken in the episode.\n# Below code will pick up 1 csv file(1 csv has one episode dialouges) and stream the data and the sentiments will be calculated by using the functions defined above"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["inputPath = \"/FileStore/tables/\"\n\ndf_episode = sqlContext.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/')\n\n\n# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\nstreamingInputDF = (\n  spark\n    .readStream                       \n    .schema(df_episode.schema)               # Set the schema of the JSON data\n    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n    .csv(inputPath)\n)\nstreamingInputDF=streamingInputDF.filter(\"episode_id is not NULL\")\nstreamingInputDF=streamingInputDF.filter(\"normalized_text is not NULL\")\n\n\n\nnew_df_episode = streamingInputDF.withColumn(\"sentiment_score\", maxUdf('normalized_text'))\n\nnew_df_episode_sentiment = new_df_episode.withColumn(\"sentiment\", maxUdf1('sentiment_score'))\n\ndisplay(new_df_episode_sentiment)\n# Same query as staticInputDF\nstreamingCountsDF = (                 \n  new_df_episode_sentiment\n    .groupBy(\n      new_df_episode_sentiment.raw_character_text,\n      new_df_episode_sentiment.sentiment)\n      .count()\n)\n\n# Is this DF actually a streaming DF?\nstreamingCountsDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"3\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#The data is now processed and sentiment is calculated as positive ,negative or neutral.\n#The shuffle.partitions =3 specifies that the data has to be partitioned into 3 categories- postive, negative and neutral\n#the data can be visualized using the counts query\n#The above code processes the streaming data into batches and processes it."],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["from time import sleep\nsleep(5) "],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%sql select raw_character_text, sentiment, count from counts \nwhere raw_character_text \nin ('Homer Simpson', 'Bart Simpson', 'Lisa Simpson', 'Marge Simpson', 'Grampa Simpson')\norder by raw_character_text, sentiment"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#The pivot table above shows the number of +ve, -ve and neutral dialogues spoken by Simpsons Family."],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%sql select raw_character_text, sentiment, count from counts \nwhere raw_character_text \nin ('Homer Simpson', 'Bart Simpson', 'Lisa Simpson', 'Marge Simpson', 'Grampa Simpson')\norder by raw_character_text, sentiment"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#Stacked Bar graph visualizations of the episodes"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["inputPath = \"/FileStore/tables/\"\n\ndf_episode = sqlContext.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/')\n\n\n# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\nstreamingInputDF = (\n  spark\n    .readStream                       \n    .schema(df_episode.schema)               # Set the schema of the JSON data\n    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n    .csv(inputPath)\n)\nstreamingInputDF=streamingInputDF.filter(\"episode_id is not NULL\")\nstreamingInputDF=streamingInputDF.filter(\"normalized_text is not NULL\")\n\n\nnew_df_episode = streamingInputDF.withColumn(\"sentiment_score\", maxUdf('normalized_text'))\n\nnew_df_episode_sentiment = new_df_episode.withColumn(\"sentiment\", maxUdf1('sentiment_score'))\ndisplay(new_df_episode_sentiment)\n# Same query as staticInputDF\nstreamingCountsDF = (                 \n  new_df_episode_sentiment\n    .groupBy(\n      new_df_episode_sentiment.episode_id,\n      new_df_episode_sentiment.sentiment)\n      .count()\n)\n\n# Is this DF actually a streaming DF?\nstreamingCountsDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"3\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts1\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["from time import sleep\nsleep(5) "],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["inputPath = \"/FileStore/tables/\"\n\ndf_episode = sqlContext.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/')\n\n\n# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\nstreamingInputDF = (\n  spark\n    .readStream                       \n    .schema(df_episode.schema)               # Set the schema of the JSON data\n    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n    .csv(inputPath)\n)\nstreamingInputDF=streamingInputDF.filter(\"episode_id is not NULL\")\nstreamingInputDF=streamingInputDF.filter(\"normalized_text is not NULL\")\n\n\nnew_df_episode = streamingInputDF.withColumn(\"sentiment_score\", maxUdf('normalized_text'))\n\nnew_df_episode_sentiment = new_df_episode.withColumn(\"sentiment\", maxUdf1('sentiment_score'))\ndisplay(new_df_episode_sentiment)\n# Same query as staticInputDF\nstreamingCountsDF = (                 \n  new_df_episode_sentiment\n    .groupBy(\n      new_df_episode_sentiment.episode_id,\n      new_df_episode_sentiment.sentiment_score,\n      new_df_episode_sentiment.sentiment)\n      .count()\n)\n\n# Is this DF actually a streaming DF?\nstreamingCountsDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"3\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts2\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["#Connclusion: A basic stuctured streaming analysis for the simpsons dataset is done.\n#We were able to understand the trends for the varying sentiments while episode is streaming and over the episodes."],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"SimpsonsNotebook","notebookId":156130183485380},"nbformat":4,"nbformat_minor":0}
